{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 18:34:51.226297: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-09 18:34:58.116535: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7fefe31ca0b8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "labels_ = ['1', '2', '3', 'space', 'del', 'A', 'B', 'C']\n",
    "NUM_CLASSES = len(labels_)\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True,\n",
    "          activation='relu', input_shape=(21, 3)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "# Create a checkpoint object and restore the weights\n",
    "checkpoint_path = \"model_checkpoints/cp_numbers_1.ckpt\"\n",
    "model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 21, 64)            17408     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 21, 128)           98816     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 264       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 172,136\n",
      "Trainable params: 172,136\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    }
   ],
   "source": [
    "saved_model_path = \"model/\"\n",
    "model.save(saved_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "loaded_model = tf.keras.models.load_model(\"model/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 21, 64)            17408     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 21, 128)           98816     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 264       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 172,136\n",
      "Trainable params: 172,136\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast_autocomplete import autocomplete_factory\n",
    "\n",
    "content_files = {\n",
    "    'words': {\n",
    "        'filepath': 'word_dict.json',\n",
    "        'compress': True  # means compress the graph data in memory\n",
    "    }\n",
    "}\n",
    "alphabet = ['1', '2', '3', 'space', 'del', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I',\n",
    "            'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "autocomplete = autocomplete_factory(content_files=content_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['and'], ['as']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autocomplete.search(word='a',size=2, max_cost=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n",
    "import numpy as np\n",
    "\n",
    "def most_frequent(List):\n",
    "    return max(set(List), key=List.count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialze bounding box to empty\n",
    "bbox = ''\n",
    "count = 0\n",
    "\n",
    "counter = 0\n",
    "\n",
    "letters = []\n",
    "\n",
    "limit_counter = 3\n",
    "\n",
    "activate = 1\n",
    "\n",
    "with mp_hands.Hands(\n",
    "        model_complexity=0,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5) as hands:\n",
    "  while True:\n",
    "      # js_reply = video_frame(label_html, bbox)\n",
    "      # if not js_reply:\n",
    "      #     break\n",
    "\n",
    "      # # convert JS response to OpenCV Image\n",
    "      # image = js_to_image(js_reply[\"img\"])\n",
    "\n",
    "      image.flags.writeable = False\n",
    "      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "      results = hands.process(image)\n",
    "\n",
    "      # Draw the hand annotations on the image.\n",
    "      image.flags.writeable = True\n",
    "      image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "      landmarks = []\n",
    "      letter = ''\n",
    "\n",
    "      if results.multi_hand_landmarks:\n",
    "          for hand_landmarks in results.multi_hand_landmarks:\n",
    "              mp_drawing.draw_landmarks(\n",
    "                  image,\n",
    "                  hand_landmarks,\n",
    "                  mp_hands.HAND_CONNECTIONS,\n",
    "                  mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                  mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "              # Get the bounding box of the hand\n",
    "              x_min, y_min, x_max, y_max = float('inf'), float(\n",
    "                  'inf'), float('-inf'), float('-inf')\n",
    "              for landmark in hand_landmarks.landmark:\n",
    "                  x, y = landmark.x, landmark.y\n",
    "                  x_min = min(x_min, x)\n",
    "                  y_min = min(y_min, y)\n",
    "                  x_max = max(x_max, x)\n",
    "                  y_max = max(y_max, y)\n",
    "\n",
    "              # Normalize the coordinates with respect to the bounding box of the hand\n",
    "              for landmark in hand_landmarks.landmark:\n",
    "                  x, y, z = landmark.x, landmark.y, landmark.z\n",
    "                  x_norm = (x - x_min) / (x_max - x_min)\n",
    "                  y_norm = (y - y_min) / (y_max - y_min)\n",
    "                  landmarks.append([x_norm, y_norm, z])\n",
    "\n",
    "          # Convert the landmarks to a feature vector\n",
    "          x_t = np.array(landmarks)\n",
    "\n",
    "          if x_t.flatten().shape[0] == 63 and counter < limit_counter and activate:\n",
    "\n",
    "            counter += 1\n",
    "            res_ = model.predict(np.array([x_t]), verbose=0)\n",
    "\n",
    "            index_class = np.argmax(res_)\n",
    "\n",
    "            letter_ = alphabet[index_class]\n",
    "\n",
    "            letters.append(letter_)\n",
    "          elif counter == limit_counter:\n",
    "            letter = most_frequent(letters)\n",
    "\n",
    "            if letter == 'space':\n",
    "              print(' ', end='')\n",
    "              print(autocomplete.search(word=letter, size=10, max_cost=100))\n",
    "            else:\n",
    "              print(letter_, end='')\n",
    "\n",
    "            counter = 0\n",
    "\n",
    "            activate = 0\n",
    "      else:\n",
    "          activate = 1\n",
    "          counter = 0\n",
    "          letters = []\n",
    "\n",
    "      label_html = letter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
